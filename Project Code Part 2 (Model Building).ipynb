{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Team Purple - Group project\n",
    "**Team members**: Jayant Maheshwari, Shubham Rishishwar, Sharath Reddy, Rajat Gaur"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Importing relevant libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "from collections import Counter\n",
    "import nltk\n",
    "import keras\n",
    "import spacy\n",
    "import functools\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from nltk.corpus import stopwords \n",
    "from nltk.tokenize import word_tokenize\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "from gensim.test.utils import common_texts, get_tmpfile\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from keras.layers.recurrent import SimpleRNN, LSTM\n",
    "from keras.layers import Flatten, Masking\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "import tensorflow as tf\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from gensim.models import Word2Vec\n",
    "from nltk import word_tokenize\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from random import randint\n",
    "from numpy import array, argmax, asarray, zeros\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.layers import Embedding\n",
    "from keras.utils import to_categorical\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.model_selection import train_test_split\n",
    "import warnings\n",
    "from sklearn import linear_model\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "nlp = spacy.load(\"en_core_web_md\") "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 0. Read inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reading 4 Data Frames from \"Project Code - Part 1.ipynb\" (used for data preprocessing)\n",
    "\n",
    "df_color = pd.read_csv(\"Primary color.csv\",index_col = 0)\n",
    "df_fit = pd.read_csv(\"fit.csv\",index_col = 0)\n",
    "df_occasion = pd.read_csv(\"occasion.csv\",index_col = 0)\n",
    "df_style = pd.read_csv(\"style.csv\", index_col = 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# transforming T/F to binary (our labels start from 6th column, so we use slicing from [6:])\n",
    "# We convert these labels from TRUE/FALSE to 1/0)\n",
    "\n",
    "df_color.iloc[:,6:]= df_color.iloc[:,6:].astype(int)\n",
    "df_fit.iloc[:,6:]= df_fit.iloc[:,6:].astype(int)\n",
    "df_occasion.iloc[:,6:]= df_occasion.iloc[:,6:].astype(int)\n",
    "df_style.iloc[:,6:]= df_style.iloc[:,6:].astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>product_id</th>\n",
       "      <th>brand</th>\n",
       "      <th>product_full_name</th>\n",
       "      <th>description</th>\n",
       "      <th>brand_category</th>\n",
       "      <th>details</th>\n",
       "      <th>blacks</th>\n",
       "      <th>pinks</th>\n",
       "      <th>whites</th>\n",
       "      <th>reds</th>\n",
       "      <th>...</th>\n",
       "      <th>grays</th>\n",
       "      <th>golds</th>\n",
       "      <th>navy</th>\n",
       "      <th>yellows</th>\n",
       "      <th>burgundies</th>\n",
       "      <th>purples</th>\n",
       "      <th>browns</th>\n",
       "      <th>multi</th>\n",
       "      <th>oranges</th>\n",
       "      <th>teal</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>01DSECZPAGJJC1EDC79JRBF4WK</td>\n",
       "      <td>Banana Republic</td>\n",
       "      <td>Mock-Neck Sweater Top</td>\n",
       "      <td>Designed worn high-waisted bottom oh-so-now mo...</td>\n",
       "      <td>Unknown</td>\n",
       "      <td>Designed worn high-waisted bottom oh-so-now mo...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>01DVA59VHYAPT4PVX32NXW91G5</td>\n",
       "      <td>Tibi</td>\n",
       "      <td>Juan Embossed Mules</td>\n",
       "      <td>Tibis Juan embossed mule made shiny black leat...</td>\n",
       "      <td>women:SHOES:MULES</td>\n",
       "      <td>seen Pre-Fall ‘19 runway Heel measure approxim...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2 rows × 25 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                   product_id            brand      product_full_name  \\\n",
       "0  01DSECZPAGJJC1EDC79JRBF4WK  Banana Republic  Mock-Neck Sweater Top   \n",
       "1  01DVA59VHYAPT4PVX32NXW91G5             Tibi    Juan Embossed Mules   \n",
       "\n",
       "                                         description     brand_category  \\\n",
       "0  Designed worn high-waisted bottom oh-so-now mo...            Unknown   \n",
       "1  Tibis Juan embossed mule made shiny black leat...  women:SHOES:MULES   \n",
       "\n",
       "                                             details  blacks  pinks  whites  \\\n",
       "0  Designed worn high-waisted bottom oh-so-now mo...       1      0       1   \n",
       "1  seen Pre-Fall ‘19 runway Heel measure approxim...       1      0       0   \n",
       "\n",
       "   reds  ...  grays  golds  navy  yellows  burgundies  purples  browns  multi  \\\n",
       "0     0  ...      0      0     0        0           0        0       0      0   \n",
       "1     0  ...      0      0     0        0           0        0       0      0   \n",
       "\n",
       "   oranges  teal  \n",
       "0        0     0  \n",
       "1        0     0  \n",
       "\n",
       "[2 rows x 25 columns]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_color.head(2)            # sample view of dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# combine multiple (relevant) columns into a single document\n",
    "\n",
    "doc_color = df_color.brand + df_color.product_full_name + df_color.description + df_color.brand_category + df_color.details\n",
    "doc_fit = df_fit.brand + df_fit .product_full_name + df_fit.description + df_fit.brand_category + df_fit.details\n",
    "doc_occasion = df_occasion.brand + df_occasion.product_full_name + df_occasion.description + df_occasion.brand_category + df_occasion.details\n",
    "doc_style = df_style.brand + df_style.product_full_name + df_style.description + df_style.brand_category + df_style.details"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Count Vectorizer with Logistic Regression model\n",
    "\n",
    "#### 42 models trained, one for each of the 42 labels, across our 4 categories, viz., 'Occasion', 'Style', 'Fit', and 'Color'\n",
    "\n",
    "> Accuracy across the models, on average, was consistently higher than the baseline accuracy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "## 1. We first Count vectorize the above document by fit_transform method\n",
    "## 2. Next, we standardize the numbres to bring them to same scale\n",
    "## 3. We split the data into train: test (75:25)\n",
    "## 4. Then, we trained the above mentioned 42 models on our data, and stored them in a list for further use\n",
    "\n",
    "def logistic_model(doc,df,columns):\n",
    "    vectorizer = CountVectorizer()\n",
    "    vectorizer = vectorizer.fit(doc)\n",
    "    X = vectorizer.transform(doc)                                            # count vectorize the input\n",
    "    X = X.toarray()\n",
    "    \n",
    "    scaler = StandardScaler()                 \n",
    "    scaler = scaler.fit(X)\n",
    "    X = scaler.transform(X)                                                  # standardize data\n",
    "    data = pd.DataFrame(X, columns=vectorizer.get_feature_names())           # Create a dataframe\n",
    "    \n",
    "    models = []\n",
    "    \n",
    "    for col in columns:\n",
    "        y = df[col].values\n",
    "        #base_accuracy = y.sum()/len(y)\n",
    "        #base_accuracy = max(base_accuracy,1-base_accuracy)\n",
    "\n",
    "        data[\"TARGET\"] = y\n",
    "\n",
    "        train_df, test_df = train_test_split(data)                           # Train-test split\n",
    "        X_train = train_df.loc[:, ~train_df.columns.isin(['TARGET'])]\n",
    "        X_test = test_df.loc[:, ~test_df.columns.isin(['TARGET'])]\n",
    "\n",
    "        y_train = train_df[\"TARGET\"]\n",
    "        y_test = test_df[\"TARGET\"]\n",
    "\n",
    "        # Logistic model\n",
    "        clf =linear_model.LogisticRegression(C=0.001,random_state=None).fit(X_train, y_train)\n",
    "\n",
    "        # Predict labels\n",
    "        #y_pred = clf.predict(X_test)\n",
    "        \n",
    "        # check accuracy\n",
    "        #acc = np.mean(y_pred == y_test)\n",
    "        \n",
    "        models.append(clf)\n",
    "    return models, vectorizer, scaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# These are the 42 labels for which models will be fitted\n",
    "\n",
    "columns_color = ['blacks','pinks','whites','reds','greens','blues','silvers','neutrals','oranges',\n",
    "                'beiges','grays','golds','navy','yellows','burgundies','purples','browns','multi','teal']\n",
    "columns_fit   = ['business casual','classic','modern','boho','glam','romantic','casual','androgynous','edgy',\n",
    "                 'retro','athleisure']\n",
    "columns_occasion = ['day to night','work','weekend','night out','vacation','coldweather','workout'] \n",
    "columns_style = ['semi-fitted','relaxed','straight / regular','fitted / tailored','oversized']\n",
    "\n",
    "columns_list = columns_color + columns_fit + columns_occasion + columns_style\n",
    "\n",
    "# call the logistic regeression function for each category\n",
    "model_color, vector_color, scaler_color = logistic_model(doc_color,df_color,columns_color)\n",
    "model_fit, vector_fit, scaler_fit = logistic_model(doc_fit,df_fit,columns_fit)\n",
    "model_occasion, vector_occasion, scaler_occasion = logistic_model(doc_occasion,df_occasion,columns_occasion)\n",
    "model_style, vector_style, scaler_style = logistic_model(doc_style,df_style,columns_style)\n",
    "\n",
    "# combine the models into a single list\n",
    "model_list = model_color+model_fit+model_occasion+model_style"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Test on new data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test data to be input here (this is dummy data):\n",
    "test_brand = \"Forever 21\"\n",
    "test_product_full_name = \"Jeans size 34 M,\"\n",
    "test_description = \"This is a slim jeans\"\n",
    "test_brand_category = \"Denim Jeans\"\n",
    "test_details = \"Blue color\"\n",
    "\n",
    "test_docs = test_brand +\" \" + test_product_full_name + \" \" + test_description + \" \" + test_brand_category +\\\n",
    "            \" \" + test_details\n",
    "\n",
    "# Remove Punctuations\n",
    "punctuation = \"!@#$%^&*()_+<>?:.,;\"  \n",
    "    \n",
    "for c in test_docs:\n",
    "    if c in punctuation:\n",
    "        test_docs = test_docs.replace(c, \"\")\n",
    "\n",
    "        \n",
    "# Remove Stopwords  \n",
    "stop_words = set(stopwords.words('english')) \n",
    "word_tokens = word_tokenize(test_docs) \n",
    "test_docs = [w for w in word_tokens if not w in stop_words] \n",
    "test_docs = [] \n",
    "for w in word_tokens: \n",
    "    if w not in stop_words: \n",
    "        test_docs.append(w)\n",
    "\n",
    "        \n",
    "# Combine all cleaned words into a list\n",
    "test_docs = [' '.join(test_docs)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Vectorize and standardize the test documnets using the models fit in Step 1.\n",
    "\n",
    "Xtest_color = vector_color.transform(test_docs)\n",
    "Xtest_color = Xtest_color.toarray()\n",
    "Xtest_color = scaler_color.transform(Xtest_color)\n",
    "Xtest_color = pd.DataFrame(Xtest_color, columns=vector_color.get_feature_names())\n",
    "\n",
    "Xtest_fit = vector_fit.transform(test_docs)\n",
    "Xtest_fit = Xtest_fit.toarray()\n",
    "Xtest_fit = scaler_fit.transform(Xtest_fit)\n",
    "Xtest_fit = pd.DataFrame(Xtest_fit, columns=vector_fit.get_feature_names())\n",
    "\n",
    "Xtest_occasion = vector_occasion.transform(test_docs)\n",
    "Xtest_occasion = Xtest_occasion.toarray()\n",
    "Xtest_occasion = scaler_occasion.transform(Xtest_occasion)\n",
    "Xtest_occasion = pd.DataFrame(Xtest_occasion, columns=vector_occasion.get_feature_names())\n",
    "\n",
    "Xtest_style = vector_style.transform(test_docs)\n",
    "Xtest_style = Xtest_style.toarray()\n",
    "Xtest_style = scaler_style.transform(Xtest_style)\n",
    "Xtest_style = pd.DataFrame(Xtest_style, columns=vector_style.get_feature_names())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Predicted labels</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>casual</th>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>day to night</th>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>weekend</th>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "              Predicted labels\n",
       "casual                       1\n",
       "day to night                 1\n",
       "weekend                      1"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Combine all labels for which [Prediction = 1] on test_data into a single dataframe\n",
    "\n",
    "y_pred = []\n",
    "for mod in model_list[:19]:                            # first 19 labels are for Category: COLOR\n",
    "    pred = list(mod.predict(Xtest_color))\n",
    "    y_pred.append(pred)\n",
    "\n",
    "for mod in model_list[19:30]:                          # next 11 labels are for Category: FIT\n",
    "    pred = list(mod.predict(Xtest_fit))\n",
    "    y_pred.append(pred)\n",
    "    \n",
    "for mod in model_list[30:37]:                          # next 7 labels are for Category: OCCASION\n",
    "    pred = list(mod.predict(Xtest_occasion))\n",
    "    y_pred.append(pred)\n",
    "    \n",
    "for mod in model_list[37:42]:                          # last 5 labels are for Category: STYLE\n",
    "    pred = list(mod.predict(Xtest_style))\n",
    "    y_pred.append(pred)    \n",
    "\n",
    "results = pd.DataFrame(y_pred, index = columns_list, columns = ['Predicted labels'])\n",
    "results[results['Predicted labels']>0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4. Other models inspected"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define all necessary functions for later use"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "# integere encoding for the documents\n",
    "def integer_encode_documents(docs, tokenizer):\n",
    "    return tokenizer.texts_to_sequences(docs)\n",
    "\n",
    "# call the standard Stanford-trained GLOVE vectors to get the weight matrix (embeddings)\n",
    "def load_glove_vectors():\n",
    "    embeddings_index = {}\n",
    "    with open('glove.6B.100d.txt', encoding = 'utf-8') as f:\n",
    "        for line in f:\n",
    "            values = line.split()\n",
    "            word = values[0]\n",
    "            coefs = asarray(values[1:], dtype='float32')\n",
    "            embeddings_index[word] = coefs\n",
    "    return embeddings_index\n",
    "\n",
    "# Fit the Recurrent Neural Network (RNN) model on data\n",
    "def make_binary_classification_rnn_model(plot=False):\n",
    "    model = Sequential()\n",
    "    model.add(Embedding(VOCAB_SIZE, 100, weights=[embedding_matrix], input_length=MAX_SEQUENCE_LENGTH, trainable=False))\n",
    "    model.add(Masking(mask_value=0.0)) # masking layer, masks any words that don't have an embedding as 0s.\n",
    "    model.add(SimpleRNN(units=64, input_shape=(1, MAX_SEQUENCE_LENGTH)))\n",
    "    model.add(Dense(16))\n",
    "    model.add(Dense(2, activation='softmax'))\n",
    "    \n",
    "    # Compile the model\n",
    "    model.compile(\n",
    "    optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "    # summarize the model\n",
    "    model.summary()\n",
    "    return model\n",
    "\n",
    "# Fit the Long Short-term Memory (LSTM) model on data\n",
    "def make_lstm_classification_model(plot = False):\n",
    "    model = Sequential()\n",
    "    model.add(Embedding(VOCAB_SIZE, 100, weights=[embedding_matrix], input_length=MAX_SEQUENCE_LENGTH, trainable=False))\n",
    "    model.add(Masking(mask_value=0.0)) # masking layer, masks any words that don't have an embedding as 0s.\n",
    "    model.add(LSTM(units=32, input_shape=(1, MAX_SEQUENCE_LENGTH)))\n",
    "    model.add(Dense(2, activation='softmax'))\n",
    "    \n",
    "    # Compile the model\n",
    "    model.compile(\n",
    "    optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "    # summarize the model\n",
    "    model.summary()\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4a. Glove + LSTM Model "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_4\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_4 (Embedding)      (None, 200, 100)          687900    \n",
      "_________________________________________________________________\n",
      "masking_4 (Masking)          (None, 200, 100)          0         \n",
      "_________________________________________________________________\n",
      "lstm_4 (LSTM)                (None, 32)                17024     \n",
      "_________________________________________________________________\n",
      "dense_4 (Dense)              (None, 2)                 66        \n",
      "=================================================================\n",
      "Total params: 704,990\n",
      "Trainable params: 17,090\n",
      "Non-trainable params: 687,900\n",
      "_________________________________________________________________\n",
      "Train on 659 samples, validate on 74 samples\n",
      "Epoch 1/5\n",
      "659/659 [==============================] - ETA: 17s - loss: 0.7708 - accuracy: 0.250 - ETA: 10s - loss: 0.7218 - accuracy: 0.421 - ETA: 7s - loss: 0.7018 - accuracy: 0.500 - ETA: 6s - loss: 0.6878 - accuracy: 0.53 - ETA: 5s - loss: 0.6811 - accuracy: 0.55 - ETA: 4s - loss: 0.6708 - accuracy: 0.57 - ETA: 3s - loss: 0.6686 - accuracy: 0.58 - ETA: 3s - loss: 0.6492 - accuracy: 0.61 - ETA: 3s - loss: 0.6431 - accuracy: 0.62 - ETA: 2s - loss: 0.6517 - accuracy: 0.61 - ETA: 2s - loss: 0.6310 - accuracy: 0.63 - ETA: 2s - loss: 0.6279 - accuracy: 0.64 - ETA: 1s - loss: 0.6177 - accuracy: 0.65 - ETA: 1s - loss: 0.6210 - accuracy: 0.65 - ETA: 1s - loss: 0.6225 - accuracy: 0.66 - ETA: 1s - loss: 0.6301 - accuracy: 0.65 - ETA: 0s - loss: 0.6376 - accuracy: 0.65 - ETA: 0s - loss: 0.6451 - accuracy: 0.64 - ETA: 0s - loss: 0.6489 - accuracy: 0.64 - ETA: 0s - loss: 0.6516 - accuracy: 0.64 - 5s 7ms/step - loss: 0.6551 - accuracy: 0.6404 - val_loss: 0.5734 - val_accuracy: 0.7297\n",
      "Epoch 2/5\n",
      "659/659 [==============================] - ETA: 4s - loss: 0.6890 - accuracy: 0.59 - ETA: 3s - loss: 0.6598 - accuracy: 0.62 - ETA: 3s - loss: 0.6189 - accuracy: 0.67 - ETA: 3s - loss: 0.6048 - accuracy: 0.68 - ETA: 2s - loss: 0.6108 - accuracy: 0.68 - ETA: 2s - loss: 0.6086 - accuracy: 0.69 - ETA: 2s - loss: 0.5990 - accuracy: 0.70 - ETA: 2s - loss: 0.5997 - accuracy: 0.70 - ETA: 2s - loss: 0.5949 - accuracy: 0.71 - ETA: 1s - loss: 0.5876 - accuracy: 0.72 - ETA: 1s - loss: 0.5927 - accuracy: 0.71 - ETA: 1s - loss: 0.5893 - accuracy: 0.71 - ETA: 1s - loss: 0.5935 - accuracy: 0.71 - ETA: 1s - loss: 0.5966 - accuracy: 0.70 - ETA: 1s - loss: 0.5955 - accuracy: 0.70 - ETA: 0s - loss: 0.6001 - accuracy: 0.69 - ETA: 0s - loss: 0.6014 - accuracy: 0.69 - ETA: 0s - loss: 0.6072 - accuracy: 0.69 - ETA: 0s - loss: 0.6119 - accuracy: 0.68 - ETA: 0s - loss: 0.6128 - accuracy: 0.68 - 4s 6ms/step - loss: 0.6130 - accuracy: 0.6874 - val_loss: 0.5862 - val_accuracy: 0.7162\n",
      "Epoch 3/5\n",
      "659/659 [==============================] - ETA: 3s - loss: 0.6084 - accuracy: 0.62 - ETA: 3s - loss: 0.6049 - accuracy: 0.64 - ETA: 3s - loss: 0.6035 - accuracy: 0.65 - ETA: 2s - loss: 0.6041 - accuracy: 0.67 - ETA: 2s - loss: 0.6197 - accuracy: 0.65 - ETA: 2s - loss: 0.6021 - accuracy: 0.68 - ETA: 2s - loss: 0.6056 - accuracy: 0.67 - ETA: 2s - loss: 0.6053 - accuracy: 0.68 - ETA: 2s - loss: 0.6053 - accuracy: 0.67 - ETA: 1s - loss: 0.5929 - accuracy: 0.69 - ETA: 1s - loss: 0.5930 - accuracy: 0.69 - ETA: 1s - loss: 0.5968 - accuracy: 0.68 - ETA: 1s - loss: 0.6036 - accuracy: 0.68 - ETA: 1s - loss: 0.6068 - accuracy: 0.67 - ETA: 1s - loss: 0.6044 - accuracy: 0.68 - ETA: 0s - loss: 0.6036 - accuracy: 0.68 - ETA: 0s - loss: 0.6079 - accuracy: 0.68 - ETA: 0s - loss: 0.6039 - accuracy: 0.68 - ETA: 0s - loss: 0.6069 - accuracy: 0.68 - ETA: 0s - loss: 0.6059 - accuracy: 0.68 - 4s 6ms/step - loss: 0.6055 - accuracy: 0.6859 - val_loss: 0.5775 - val_accuracy: 0.7162\n",
      "Epoch 4/5\n",
      "659/659 [==============================] - ETA: 3s - loss: 0.5801 - accuracy: 0.68 - ETA: 3s - loss: 0.5792 - accuracy: 0.73 - ETA: 3s - loss: 0.5923 - accuracy: 0.70 - ETA: 3s - loss: 0.5897 - accuracy: 0.70 - ETA: 2s - loss: 0.6189 - accuracy: 0.66 - ETA: 2s - loss: 0.6052 - accuracy: 0.69 - ETA: 2s - loss: 0.6067 - accuracy: 0.69 - ETA: 2s - loss: 0.5975 - accuracy: 0.69 - ETA: 2s - loss: 0.5920 - accuracy: 0.69 - ETA: 2s - loss: 0.5842 - accuracy: 0.70 - ETA: 1s - loss: 0.5764 - accuracy: 0.71 - ETA: 1s - loss: 0.5888 - accuracy: 0.69 - ETA: 1s - loss: 0.5870 - accuracy: 0.70 - ETA: 1s - loss: 0.6000 - accuracy: 0.68 - ETA: 1s - loss: 0.5982 - accuracy: 0.68 - ETA: 0s - loss: 0.5968 - accuracy: 0.69 - ETA: 0s - loss: 0.5961 - accuracy: 0.69 - ETA: 0s - loss: 0.5978 - accuracy: 0.68 - ETA: 0s - loss: 0.5995 - accuracy: 0.68 - ETA: 0s - loss: 0.6008 - accuracy: 0.68 - 4s 6ms/step - loss: 0.5994 - accuracy: 0.6889 - val_loss: 0.5797 - val_accuracy: 0.7162\n",
      "Epoch 5/5\n",
      "659/659 [==============================] - ETA: 4s - loss: 0.6003 - accuracy: 0.68 - ETA: 4s - loss: 0.6097 - accuracy: 0.68 - ETA: 4s - loss: 0.5698 - accuracy: 0.72 - ETA: 3s - loss: 0.5950 - accuracy: 0.69 - ETA: 3s - loss: 0.5754 - accuracy: 0.70 - ETA: 3s - loss: 0.5864 - accuracy: 0.68 - ETA: 3s - loss: 0.5856 - accuracy: 0.69 - ETA: 2s - loss: 0.5905 - accuracy: 0.68 - ETA: 2s - loss: 0.5742 - accuracy: 0.71 - ETA: 2s - loss: 0.5958 - accuracy: 0.68 - ETA: 2s - loss: 0.5957 - accuracy: 0.68 - ETA: 1s - loss: 0.5943 - accuracy: 0.69 - ETA: 1s - loss: 0.5873 - accuracy: 0.69 - ETA: 1s - loss: 0.5829 - accuracy: 0.70 - ETA: 1s - loss: 0.5870 - accuracy: 0.69 - ETA: 1s - loss: 0.5978 - accuracy: 0.68 - ETA: 0s - loss: 0.5928 - accuracy: 0.69 - ETA: 0s - loss: 0.5894 - accuracy: 0.69 - ETA: 0s - loss: 0.5882 - accuracy: 0.69 - ETA: 0s - loss: 0.5878 - accuracy: 0.69 - 5s 7ms/step - loss: 0.5921 - accuracy: 0.6920 - val_loss: 0.5769 - val_accuracy: 0.7162\n",
      "184/184 [==============================] - ETA:  - ETA:  - ETA:  - 0s 1ms/step\n",
      "Accuracy: 64.130437\n"
     ]
    }
   ],
   "source": [
    "# Tokenize Text\n",
    "tokenizer = Tokenizer(num_words=5000, oov_token=\"UNKNOWN_TOKEN\")\n",
    "tokenizer.fit_on_texts(list(doc_color))\n",
    "\n",
    "# integer encode the documents\n",
    "encoded_docs = integer_encode_documents(doc_color, tokenizer)\n",
    "\n",
    "# padding to create equal length sequences\n",
    "MAX_SEQUENCE_LENGTH = 200\n",
    "padded_docs = tf.keras.preprocessing.sequence.pad_sequences(encoded_docs, maxlen=MAX_SEQUENCE_LENGTH, padding='post')\n",
    "\n",
    "encoder = LabelEncoder()\n",
    "labels = to_categorical(encoder.fit_transform(df_color['blacks']))\n",
    "\n",
    "# train-test split\n",
    "X_train, X_test, y_train, y_test = train_test_split(padded_docs, labels, test_size=0.2)\n",
    "\n",
    "VOCAB_SIZE = int(len(tokenizer.word_index) * 1.1)\n",
    "\n",
    "# Load in GloVe Vectors\n",
    "embeddings_index = load_glove_vectors()\n",
    "embeddings_index\n",
    "\n",
    "# # create a weight matrix for words in training docs\n",
    "embedding_matrix = zeros((VOCAB_SIZE, 100))\n",
    "for word, i in tokenizer.word_index.items():\n",
    "    embedding_vector = embeddings_index.get(word)\n",
    "    if embedding_vector is not None: # check that it is an actual word that we have embeddings for\n",
    "        embedding_matrix[i] = embedding_vector\n",
    "        \n",
    "# define model\n",
    "model = make_lstm_classification_model()\n",
    "\n",
    "# fit the model\n",
    "history = model.fit(X_train, y_train,validation_split = 0.1, epochs=5, verbose=1)\n",
    "\n",
    "# evaluate the model\n",
    "loss, accuracy = model.evaluate(X_test, y_test, verbose=1)\n",
    "print('Accuracy: %f' % (accuracy*100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Baseline accuracy for label Black (sample label): 67.61177753544165 %\n",
      "LSTM Model accuracy for label Black (sample label): 66.847825050354 %\n"
     ]
    }
   ],
   "source": [
    "print('Baseline accuracy for label Black (sample label):', 100*(1 - df_color['blacks'].sum()/len(df_color['blacks'])), '%')\n",
    "print('LSTM Model accuracy for label Black (sample label):', 100*accuracy, '%')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 0, 0, 0])"
      ]
     },
     "execution_count": 101,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# WE DID NOT select this model, as accuracy was similar to baseline accuracy \n",
    "## Test on new data\n",
    "\n",
    "test_docs = [\n",
    "    \"Employees look like they hate their job. Milkshake was like drinking milk. Food was cold and not warm at all\",\n",
    "    \"This Mcdonalds is not only in the business of making crappy food and providing even crappier service watch out for the racket they have in the parking lot . If your not careful reading the sign at the the front of the entrance it is going to cost you $195.00 in parking fees. went in to to ask the management they just blew me off. lucky they are in vegas where they dont count on repeat businesssss.\",\n",
    "    \"There are better stores without fruit flies in Griffin, GA.\",\n",
    "    \"Slowest drive-thru ever. Better option is to go to the location on arlington\"\n",
    "]\n",
    "\n",
    "test_docs = list(\n",
    "    map(lambda doc: \" \".join([token.text for token in nlp(doc) if not token.is_stop]), test_docs))\n",
    "\n",
    "encoded_test_sample = integer_encode_documents(test_docs, tokenizer)\n",
    "\n",
    "padded_test_docs = keras.preprocessing.sequence.pad_sequences(encoded_test_sample, maxlen=MAX_SEQUENCE_LENGTH, padding='post')\n",
    "\n",
    "model.predict_classes(padded_test_docs)\n",
    "prediction = model.predict_classes(padded_test_docs)\n",
    "encoder.inverse_transform(prediction)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4b. Glove + RNN Model "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_5\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_5 (Embedding)      (None, 200, 100)          687900    \n",
      "_________________________________________________________________\n",
      "masking_5 (Masking)          (None, 200, 100)          0         \n",
      "_________________________________________________________________\n",
      "simple_rnn_1 (SimpleRNN)     (None, 64)                10560     \n",
      "_________________________________________________________________\n",
      "dense_5 (Dense)              (None, 16)                1040      \n",
      "_________________________________________________________________\n",
      "dense_6 (Dense)              (None, 2)                 34        \n",
      "=================================================================\n",
      "Total params: 699,534\n",
      "Trainable params: 11,634\n",
      "Non-trainable params: 687,900\n",
      "_________________________________________________________________\n",
      "Train on 659 samples, validate on 74 samples\n",
      "Epoch 1/5\n",
      "659/659 [==============================] - ETA: 13s - loss: 1.5493 - accuracy: 0.250 - ETA: 7s - loss: 1.3049 - accuracy: 0.343 - ETA: 5s - loss: 1.1357 - accuracy: 0.39 - ETA: 4s - loss: 1.0718 - accuracy: 0.41 - ETA: 3s - loss: 1.0451 - accuracy: 0.45 - ETA: 3s - loss: 0.9909 - accuracy: 0.47 - ETA: 2s - loss: 0.9918 - accuracy: 0.50 - ETA: 2s - loss: 1.0052 - accuracy: 0.51 - ETA: 2s - loss: 1.0508 - accuracy: 0.52 - ETA: 1s - loss: 1.0266 - accuracy: 0.53 - ETA: 1s - loss: 1.0025 - accuracy: 0.54 - ETA: 1s - loss: 0.9859 - accuracy: 0.55 - ETA: 1s - loss: 0.9736 - accuracy: 0.56 - ETA: 0s - loss: 0.9672 - accuracy: 0.55 - ETA: 0s - loss: 0.9579 - accuracy: 0.56 - ETA: 0s - loss: 0.9565 - accuracy: 0.55 - ETA: 0s - loss: 0.9419 - accuracy: 0.56 - ETA: 0s - loss: 0.9313 - accuracy: 0.56 - ETA: 0s - loss: 0.9143 - accuracy: 0.57 - ETA: 0s - loss: 0.9053 - accuracy: 0.57 - 3s 5ms/step - loss: 0.9028 - accuracy: 0.5706 - val_loss: 0.6949 - val_accuracy: 0.5541\n",
      "Epoch 2/5\n",
      "659/659 [==============================] - ETA: 2s - loss: 0.7238 - accuracy: 0.53 - ETA: 2s - loss: 0.6308 - accuracy: 0.64 - ETA: 1s - loss: 0.6246 - accuracy: 0.66 - ETA: 1s - loss: 0.6568 - accuracy: 0.64 - ETA: 1s - loss: 0.6486 - accuracy: 0.65 - ETA: 1s - loss: 0.6511 - accuracy: 0.65 - ETA: 1s - loss: 0.6529 - accuracy: 0.65 - ETA: 1s - loss: 0.6514 - accuracy: 0.64 - ETA: 1s - loss: 0.6657 - accuracy: 0.63 - ETA: 1s - loss: 0.6641 - accuracy: 0.63 - ETA: 1s - loss: 0.6667 - accuracy: 0.63 - ETA: 0s - loss: 0.6702 - accuracy: 0.62 - ETA: 0s - loss: 0.6741 - accuracy: 0.62 - ETA: 0s - loss: 0.6732 - accuracy: 0.63 - ETA: 0s - loss: 0.6697 - accuracy: 0.63 - ETA: 0s - loss: 0.6588 - accuracy: 0.64 - ETA: 0s - loss: 0.6548 - accuracy: 0.65 - ETA: 0s - loss: 0.6527 - accuracy: 0.65 - ETA: 0s - loss: 0.6546 - accuracy: 0.64 - ETA: 0s - loss: 0.6516 - accuracy: 0.64 - 2s 4ms/step - loss: 0.6482 - accuracy: 0.6495 - val_loss: 0.6355 - val_accuracy: 0.6081\n",
      "Epoch 3/5\n",
      "659/659 [==============================] - ETA: 2s - loss: 0.4793 - accuracy: 0.78 - ETA: 1s - loss: 0.5200 - accuracy: 0.76 - ETA: 1s - loss: 0.5500 - accuracy: 0.75 - ETA: 1s - loss: 0.5493 - accuracy: 0.74 - ETA: 1s - loss: 0.5565 - accuracy: 0.73 - ETA: 1s - loss: 0.5898 - accuracy: 0.70 - ETA: 1s - loss: 0.5937 - accuracy: 0.70 - ETA: 1s - loss: 0.5859 - accuracy: 0.70 - ETA: 1s - loss: 0.5912 - accuracy: 0.70 - ETA: 1s - loss: 0.5930 - accuracy: 0.70 - ETA: 1s - loss: 0.5880 - accuracy: 0.70 - ETA: 0s - loss: 0.5867 - accuracy: 0.70 - ETA: 0s - loss: 0.5825 - accuracy: 0.70 - ETA: 0s - loss: 0.5811 - accuracy: 0.70 - ETA: 0s - loss: 0.5919 - accuracy: 0.69 - ETA: 0s - loss: 0.5885 - accuracy: 0.70 - ETA: 0s - loss: 0.5966 - accuracy: 0.69 - ETA: 0s - loss: 0.5916 - accuracy: 0.68 - ETA: 0s - loss: 0.5929 - accuracy: 0.68 - ETA: 0s - loss: 0.5943 - accuracy: 0.68 - 2s 4ms/step - loss: 0.5923 - accuracy: 0.6844 - val_loss: 0.6193 - val_accuracy: 0.6757\n",
      "Epoch 4/5\n",
      "659/659 [==============================] - ETA: 2s - loss: 0.4985 - accuracy: 0.81 - ETA: 2s - loss: 0.4976 - accuracy: 0.75 - ETA: 1s - loss: 0.5150 - accuracy: 0.73 - ETA: 1s - loss: 0.5211 - accuracy: 0.71 - ETA: 1s - loss: 0.5361 - accuracy: 0.70 - ETA: 1s - loss: 0.5311 - accuracy: 0.70 - ETA: 1s - loss: 0.5414 - accuracy: 0.70 - ETA: 1s - loss: 0.5323 - accuracy: 0.71 - ETA: 1s - loss: 0.5226 - accuracy: 0.72 - ETA: 1s - loss: 0.5321 - accuracy: 0.71 - ETA: 1s - loss: 0.5332 - accuracy: 0.71 - ETA: 0s - loss: 0.5292 - accuracy: 0.71 - ETA: 0s - loss: 0.5264 - accuracy: 0.72 - ETA: 0s - loss: 0.5353 - accuracy: 0.71 - ETA: 0s - loss: 0.5424 - accuracy: 0.70 - ETA: 0s - loss: 0.5393 - accuracy: 0.71 - ETA: 0s - loss: 0.5452 - accuracy: 0.70 - ETA: 0s - loss: 0.5552 - accuracy: 0.70 - ETA: 0s - loss: 0.5593 - accuracy: 0.70 - ETA: 0s - loss: 0.5609 - accuracy: 0.70 - 2s 4ms/step - loss: 0.5616 - accuracy: 0.7026 - val_loss: 0.6669 - val_accuracy: 0.6081\n",
      "Epoch 5/5\n",
      "659/659 [==============================] - ETA: 2s - loss: 0.4426 - accuracy: 0.81 - ETA: 2s - loss: 0.4665 - accuracy: 0.79 - ETA: 1s - loss: 0.5039 - accuracy: 0.79 - ETA: 1s - loss: 0.5073 - accuracy: 0.76 - ETA: 1s - loss: 0.4877 - accuracy: 0.80 - ETA: 1s - loss: 0.4946 - accuracy: 0.79 - ETA: 1s - loss: 0.5186 - accuracy: 0.76 - ETA: 1s - loss: 0.5092 - accuracy: 0.77 - ETA: 1s - loss: 0.5112 - accuracy: 0.77 - ETA: 1s - loss: 0.5215 - accuracy: 0.76 - ETA: 1s - loss: 0.5312 - accuracy: 0.75 - ETA: 0s - loss: 0.5370 - accuracy: 0.74 - ETA: 0s - loss: 0.5331 - accuracy: 0.74 - ETA: 0s - loss: 0.5287 - accuracy: 0.74 - ETA: 0s - loss: 0.5252 - accuracy: 0.75 - ETA: 0s - loss: 0.5264 - accuracy: 0.75 - ETA: 0s - loss: 0.5301 - accuracy: 0.74 - ETA: 0s - loss: 0.5319 - accuracy: 0.74 - ETA: 0s - loss: 0.5306 - accuracy: 0.74 - ETA: 0s - loss: 0.5325 - accuracy: 0.74 - 2s 4ms/step - loss: 0.5338 - accuracy: 0.7436 - val_loss: 0.6351 - val_accuracy: 0.6486\n",
      "184/184 [==============================] - ETA:  - ETA:  - 0s 743us/step\n",
      "Accuracy: 66.847825\n"
     ]
    }
   ],
   "source": [
    "# Tokenize Text\n",
    "tokenizer = Tokenizer(num_words=5000, oov_token=\"UNKNOWN_TOKEN\")\n",
    "tokenizer.fit_on_texts(list(doc_color))\n",
    "\n",
    "# integer encode the documents\n",
    "encoded_docs = integer_encode_documents(doc_color, tokenizer)\n",
    "\n",
    "# padding to create equal length sequences\n",
    "MAX_SEQUENCE_LENGTH = 200\n",
    "padded_docs = tf.keras.preprocessing.sequence.pad_sequences(encoded_docs, maxlen=MAX_SEQUENCE_LENGTH, padding='post')\n",
    "\n",
    "encoder = LabelEncoder()\n",
    "labels = to_categorical(encoder.fit_transform(df_color['blacks']))\n",
    "\n",
    "# train-test split\n",
    "X_train, X_test, y_train, y_test = train_test_split(padded_docs, labels, test_size=0.2)\n",
    "\n",
    "VOCAB_SIZE = int(len(tokenizer.word_index) * 1.1)\n",
    "\n",
    "# Load in GloVe Vectors\n",
    "embeddings_index = load_glove_vectors()\n",
    "embeddings_index\n",
    "\n",
    "# # create a weight matrix for words in training docs\n",
    "embedding_matrix = zeros((VOCAB_SIZE, 100))\n",
    "for word, i in tokenizer.word_index.items():\n",
    "    embedding_vector = embeddings_index.get(word)\n",
    "    if embedding_vector is not None: # check that it is an actual word that we have embeddings for\n",
    "        embedding_matrix[i] = embedding_vector\n",
    "        \n",
    "# define model\n",
    "model = make_binary_classification_rnn_model()\n",
    "\n",
    "# fit the model\n",
    "history = model.fit(X_train, y_train,validation_split = 0.1, epochs=5, verbose=1)\n",
    "\n",
    "# evaluate the model\n",
    "loss, accuracy = model.evaluate(X_test, y_test, verbose=1)\n",
    "print('Accuracy: %f' % (accuracy*100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Baseline accuracy for label Black (sample label): 67.61177753544165 %\n",
      "RNN Model accuracy for label Black (sample label): 66.847825050354 %\n"
     ]
    }
   ],
   "source": [
    "print('Baseline accuracy for label Black (sample label):', 100*(1 - df_color['blacks'].sum()/len(df_color['blacks'])), '%')\n",
    "print('RNN Model accuracy for label Black (sample label):', 100*accuracy, '%')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Work in progress\n",
    "\n",
    "### LSTM + Word2Vec (Equal Weights)\n",
    "\n",
    "**unable to finally generate any meaningful output**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tokenize Text\n",
    "tokenizer = Tokenizer(num_words=5000, oov_token=\"UNKNOWN_TOKEN\")\n",
    "tokenizer.fit_on_texts(list(doc_color))\n",
    "\n",
    "# integer encode the documents\n",
    "encoded_docs = integer_encode_documents(doc_color, tokenizer)\n",
    "\n",
    "# padding to create equal length sequences\n",
    "MAX_SEQUENCE_LENGTH = 1000\n",
    "padded_docs = tf.keras.preprocessing.sequence.pad_sequences(encoded_docs, maxlen=MAX_SEQUENCE_LENGTH, padding='post')\n",
    "\n",
    "encoder = LabelEncoder()\n",
    "labels = to_categorical(encoder.fit_transform(df_color['blacks']))\n",
    "\n",
    "# train-test split\n",
    "X_train, X_test, y_train, y_test = train_test_split(padded_docs, labels, test_size=0.2)\n",
    "\n",
    "VOCAB_SIZE = int(len(tokenizer.word_index) * 1.1)\n",
    "\n",
    "# Load in GloVe Vectors\n",
    "embedding_matrix = []\n",
    "for i in doc_color:\n",
    "    embedding_matrix.append(nlp(i).vector)\n",
    "embedding_matrix = np.asarray(embedding_matrix)\n",
    "\n",
    "# embeddings_index = load_glove_vectors()\n",
    "# embeddings_index\n",
    "\n",
    "#create a weight matrix for words in training docs\n",
    "# embedding_matrix = zeros((VOCAB_SIZE, 100))\n",
    "# for word, i in tokenizer.word_index.items():\n",
    "#     embedding_vector = embeddings_index.get(word)\n",
    "#     if embedding_vector is not None: # check that it is an actual word that we have embeddings for\n",
    "#         embedding_matrix[i] = embedding_vector\n",
    "\n",
    "\n",
    "# define model\n",
    "model = make_lstm_classification_model2()\n",
    "\n",
    "# fit the model\n",
    "history = model.fit(X_train, y_train,validation_split = 0.1, epochs=5, verbose=1)\n",
    "\n",
    "# evaluate the model\n",
    "loss, accuracy = model.evaluate(X_test, y_test, verbose=1)\n",
    "print('Accuracy: %f' % (accuracy*100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp(doc).vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test = \"black shoes green belt\"\n",
    "# nlp(test).vector"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Self Trained Corpus\n",
    "Generate word vectors by self-training the weights on corpus "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [],
   "source": [
    "doc = list(doc.values)\n",
    "doc = [word_tokenize(review) for review in doc]\n",
    "model = Word2Vec(doc, min_count=5)\n",
    "words = list(model.wv.vocab)\n",
    "vectors = []\n",
    "for word in words:\n",
    "    vectors.append(model[word].tolist())\n",
    "data = np.array(vectors)\n",
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
